{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans input text by removing URLs, non-alphabetic characters, \n",
    "    converting to lowercase, and removing stopwords.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    text = text.lower()\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('content_with_logic.csv')\n",
    "\n",
    "# Combine Caption, Hashtags, and Comments into one field\n",
    "df['content'] = df['Caption'].fillna(\n",
    "    '') + ' ' + df['Hashtags'].fillna('') + ' ' + df['Comment_Text'].fillna('')\n",
    "\n",
    "# Clean content\n",
    "df['clean_content'] = df['content'].apply(clean_text)\n",
    "\n",
    "# Tokenize cleaned content\n",
    "df['tokenized_content'] = df['clean_content'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('content_with_logic.csv')\n",
    "\n",
    "# Combine Caption, Hashtags, and Comments into one field\n",
    "df['content'] = df['Caption'].fillna(\n",
    "    '') + ' ' + df['Hashtags'].fillna('') + ' ' + df['Comment_Text'].fillna('')\n",
    "\n",
    "# Clean content\n",
    "df['clean_content'] = df['content'].apply(clean_text)\n",
    "\n",
    "# Tokenize cleaned content\n",
    "df['tokenized_content'] = df['clean_content'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=df['tokenized_content'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "word2vec_model.train(df['tokenized_content'], total_examples=len(\n",
    "    df['tokenized_content']), epochs=10)\n",
    "\n",
    "# Get average Word2Vec embeddings for each post\n",
    "\n",
    "\n",
    "def get_avg_word2vec(tokens):\n",
    "    vectors = [word2vec_model.wv[word]\n",
    "               for word in tokens if word in word2vec_model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(100)  # Return zero vector if no words are found\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "\n",
    "df['content_embeddings'] = df['tokenized_content'].apply(get_avg_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = tfidf.fit_transform(df['clean_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Engagement_Score and Sentiment_Score columns exist\n",
    "df['Engagement_Score'] = pd.to_numeric(\n",
    "    df.get('Engagement_Score', 0), errors='coerce').fillna(0)\n",
    "df['Sentiment_Score'] = pd.to_numeric(\n",
    "    df.get('Sentiment_Score', 0), errors='coerce').fillna(0)\n",
    "\n",
    "# Calculate weighted score (70% engagement, 30% sentiment)\n",
    "df['weighted_score'] = df['Engagement_Score'] * \\\n",
    "    0.7 + df['Sentiment_Score'] * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_posts(user_id, user_profiles):\n",
    "    \"\"\"\n",
    "    Recommend posts for a user based on content similarity and weighted scores.\n",
    "    \"\"\"\n",
    "    if user_id in user_profiles:\n",
    "        user_profile_vector = user_profiles[user_id].reshape(1, -1)\n",
    "\n",
    "        # Calculate cosine similarity between user profile and posts\n",
    "        cosine_similarities = cosine_similarity(\n",
    "            user_profile_vector, np.array(df['content_embeddings'].tolist()))\n",
    "\n",
    "        # Rank posts by cosine similarity\n",
    "        recommended_post_indices = np.argsort(\n",
    "            cosine_similarities[0])[-10:]  # Top 10 recommendations\n",
    "        recommended_posts = df.iloc[recommended_post_indices]\n",
    "\n",
    "        # Re-rank posts based on weighted score\n",
    "        final_recommendations = recommended_posts.sort_values(\n",
    "            by='weighted_score', ascending=False)\n",
    "        return final_recommendations[['post_id', 'Caption', 'Hashtags', 'weighted_score']]\n",
    "    else:\n",
    "        print(f\"No profile found for user_id {user_id}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(recommended_posts, true_interactions, k):\n",
    "    relevant = set(true_interactions[:k])\n",
    "    recommended = set(recommended_posts[:k])\n",
    "    return len(relevant.intersection(recommended)) / k\n",
    "\n",
    "\n",
    "def recall_at_k(recommended_posts, true_interactions, k):\n",
    "    relevant = set(true_interactions[:k])\n",
    "    recommended = set(recommended_posts[:k])\n",
    "    return len(relevant.intersection(relevant)) / len(relevant)\n",
    "\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save user profiles for future use\n",
    "with open('content.pkl', 'wb') as f:\n",
    "    pickle.dump(user_profiles, f)\n",
    "\n",
    "# Load saved profiles if needed\n",
    "with open('content.pkl', 'rb') as f:\n",
    "    user_profiles = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>Caption</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>weighted_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1406</td>\n",
       "      <td>Arm provide music letter local record else abi...</td>\n",
       "      <td>technology it team pull</td>\n",
       "      <td>355.813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>7313</td>\n",
       "      <td>Evening radio professional again interview sam...</td>\n",
       "      <td>trip</td>\n",
       "      <td>240.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>8174</td>\n",
       "      <td>Number radio pay western sound section authori...</td>\n",
       "      <td>though education rich industry</td>\n",
       "      <td>226.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>8209</td>\n",
       "      <td>Mind east attorney very industry manager week ...</td>\n",
       "      <td>difficult bank item prepare seek</td>\n",
       "      <td>207.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>4227</td>\n",
       "      <td>Generation memory economy agreement finish cho...</td>\n",
       "      <td>positive peace machine</td>\n",
       "      <td>207.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>5154</td>\n",
       "      <td>Occur material wish wrong once hard wind despi...</td>\n",
       "      <td>attention manage</td>\n",
       "      <td>205.505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>8208</td>\n",
       "      <td>Near cut week business service position contai...</td>\n",
       "      <td>hear agree</td>\n",
       "      <td>204.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>3873</td>\n",
       "      <td>Hospital mind live late along benefit expert s...</td>\n",
       "      <td>rich short including hand</td>\n",
       "      <td>176.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>3377</td>\n",
       "      <td>Project sing minute manager operation region s...</td>\n",
       "      <td>former line</td>\n",
       "      <td>143.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>8011</td>\n",
       "      <td>Through give class wear by oil control five te...</td>\n",
       "      <td>agent</td>\n",
       "      <td>137.063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     post_id                                            Caption  \\\n",
       "495     1406  Arm provide music letter local record else abi...   \n",
       "571     7313  Evening radio professional again interview sam...   \n",
       "741     8174  Number radio pay western sound section authori...   \n",
       "694     8209  Mind east attorney very industry manager week ...   \n",
       "302     4227  Generation memory economy agreement finish cho...   \n",
       "668     5154  Occur material wish wrong once hard wind despi...   \n",
       "194     8208  Near cut week business service position contai...   \n",
       "397     3873  Hospital mind live late along benefit expert s...   \n",
       "401     3377  Project sing minute manager operation region s...   \n",
       "784     8011  Through give class wear by oil control five te...   \n",
       "\n",
       "                             Hashtags  weighted_score  \n",
       "495           technology it team pull         355.813  \n",
       "571                              trip         240.180  \n",
       "741    though education rich industry         226.810  \n",
       "694  difficult bank item prepare seek         207.210  \n",
       "302            positive peace machine         207.192  \n",
       "668                  attention manage         205.505  \n",
       "194                        hear agree         204.185  \n",
       "397         rich short including hand         176.130  \n",
       "401                       former line         143.562  \n",
       "784                             agent         137.063  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_posts(552, user_profiles)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
