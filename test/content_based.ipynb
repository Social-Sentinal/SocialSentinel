{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data import info_url,posts_url,reels_url,hashtags_url\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collected data from different API calls\n",
    "info_data = requests.get(info, headers=headers, params=querystring).json()\n",
    "posts_data = requests.get(post, headers=headers, params=querystring).json()\n",
    "reels_data = requests.get(reels, headers=headers, params=querystring).json()\n",
    "hashtags_data = requests.get(\n",
    "    hastag, headers=headers, params=querystring).json()\n",
    "\n",
    "# Converting the collected data into DataFrames\n",
    "info_df = pd.DataFrame(info_data['data'])\n",
    "posts_df = pd.DataFrame(posts_data['data'])\n",
    "reels_df = pd.DataFrame(reels_data['data'])\n",
    "hashtags_df = pd.DataFrame(hashtags_data['data'])\n",
    "\n",
    "# Save the datasets if needed\n",
    "info_df.to_csv(\"info_data.csv\", index=False)\n",
    "posts_df.to_csv(\"posts_data.csv\", index=False)\n",
    "reels_df.to_csv(\"reels_data.csv\", index=False)\n",
    "hashtags_df.to_csv(\"hashtags_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "posts_df.drop_duplicates(inplace=True)\n",
    "reels_df.drop_duplicates(inplace=True)\n",
    "hashtags_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values or remove rows with missing data\n",
    "posts_df.fillna('', inplace=True)\n",
    "reels_df.fillna('', inplace=True)\n",
    "hashtags_df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Removing URLs and special characters\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "posts_df['caption'] = posts_df['caption'].apply(clean_text)\n",
    "reels_df['caption'] = reels_df['caption'].apply(clean_text)\n",
    "hashtags_df['hashtags'] = hashtags_df['hashtags'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all datasets into a single DataFrame\n",
    "final_data = pd.merge(posts_df[['caption', 'url']], reels_df[[\n",
    "                      'caption', 'url']], on='url', how='outer')\n",
    "final_data = pd.merge(\n",
    "    final_data, hashtags_df[['hashtags', 'url']], on='url', how='outer')\n",
    "\n",
    "# Combine captions and hashtags for a complete content-based recommendation\n",
    "final_data['content'] = final_data['caption'] + \" \" + final_data['hashtags']\n",
    "\n",
    "# Drop any remaining duplicates or nulls\n",
    "final_data.drop_duplicates(subset='url', inplace=True)\n",
    "final_data.dropna(subset=['content'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional because hamne pahle hi nikala hai\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "def get_sentiment(text):\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    return score['compound']\n",
    "\n",
    "\n",
    "final_data['sentiment'] = final_data['content'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorizing the text content\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(final_data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to recommend based on content\n",
    "def get_recommendations(index, cosine_sim=cosine_sim):\n",
    "    # Get the pairwise similarity scores of all content\n",
    "    sim_scores = list(enumerate(cosine_sim[index]))\n",
    "\n",
    "    # Sort content based on similarity score\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the indices of the top 10 most similar content\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Return the URLs of the most similar content\n",
    "    content_indices = [i[0] for i in sim_scores]\n",
    "    return final_data['url'].iloc[content_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the recommendation system for the first post\n",
    "print(get_recommendations(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "# Save the cosine similarity matrix\n",
    "with open('cosine_similarity.pkl', 'wb') as f:\n",
    "    pickle.dump(cosine_sim, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  post_id            Username  \\\n",
      "0      617      617          angeladiaz   \n",
      "1      835      835        wallacediane   \n",
      "2      554      554  johnsonchristopher   \n",
      "3      569      569            coxbryan   \n",
      "4      802      802              msmith   \n",
      "\n",
      "                                             Caption  \\\n",
      "0  Fall this water will subject stage issue usual...   \n",
      "1             Language family control teach exactly.   \n",
      "2                             Become here recognize.   \n",
      "3  We find according month green able program by ...   \n",
      "4          Car issue current movie during authority.   \n",
      "\n",
      "                       Hashtags  Likes  Comments  \n",
      "0    record his need good whole    517       144  \n",
      "1       stage animal too of lot    441       182  \n",
      "2                         story     41       197  \n",
      "3  might decide pattern provide     41       190  \n",
      "4                        impact    367       164  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Function to create synthetic dataset\n",
    "\n",
    "\n",
    "def create_synthetic_data(num_samples=1000):\n",
    "    data = {\n",
    "        'user_id': [],\n",
    "        'post_id': [],\n",
    "        'Username': [],\n",
    "        'Caption': [],\n",
    "        'Hashtags': [],\n",
    "        'Likes': [],\n",
    "        'Comments': []\n",
    "    }\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # Generate user data\n",
    "        user_id = random.randint(0, 1000)\n",
    "        post_id = random.randint(0, 1000)\n",
    "        username = fake.user_name()\n",
    "        caption = fake.sentence(nb_words=random.randint(5, 15))\n",
    "        hashtags = ' '.join([fake.word() for _ in range(random.randint(1, 5))])\n",
    "        likes = random.randint(0, 1000)\n",
    "        comments = random.randint(0, 200)\n",
    "\n",
    "        # Append data\n",
    "        data['user_id'].append(user_id)\n",
    "        data['post_id'].append(user_id)\n",
    "        data['Username'].append(username)\n",
    "        data['Caption'].append(caption)\n",
    "        data['Hashtags'].append(hashtags)\n",
    "        data['Likes'].append(likes)\n",
    "        data['Comments'].append(comments)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Create synthetic dataset\n",
    "synthetic_data = create_synthetic_data(num_samples=1000)\n",
    "\n",
    "# Save to CSV\n",
    "synthetic_data.to_csv('sentiments.csv', index=False)\n",
    "print(synthetic_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>Username</th>\n",
       "      <th>Caption</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>617</td>\n",
       "      <td>617</td>\n",
       "      <td>angeladiaz</td>\n",
       "      <td>Fall this water will subject stage issue usual...</td>\n",
       "      <td>record his need good whole</td>\n",
       "      <td>517</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>835</td>\n",
       "      <td>835</td>\n",
       "      <td>wallacediane</td>\n",
       "      <td>Language family control teach exactly.</td>\n",
       "      <td>stage animal too of lot</td>\n",
       "      <td>441</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>554</td>\n",
       "      <td>554</td>\n",
       "      <td>johnsonchristopher</td>\n",
       "      <td>Become here recognize.</td>\n",
       "      <td>story</td>\n",
       "      <td>41</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>coxbryan</td>\n",
       "      <td>We find according month green able program by ...</td>\n",
       "      <td>might decide pattern provide</td>\n",
       "      <td>41</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>802</td>\n",
       "      <td>802</td>\n",
       "      <td>msmith</td>\n",
       "      <td>Car issue current movie during authority.</td>\n",
       "      <td>impact</td>\n",
       "      <td>367</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>352</td>\n",
       "      <td>352</td>\n",
       "      <td>daviseric</td>\n",
       "      <td>Dinner whether girl boy course recognize billi...</td>\n",
       "      <td>production main lead example</td>\n",
       "      <td>355</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>953</td>\n",
       "      <td>953</td>\n",
       "      <td>shannonserrano</td>\n",
       "      <td>Son computer myself protect attention city cup...</td>\n",
       "      <td>east reduce</td>\n",
       "      <td>611</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>485</td>\n",
       "      <td>485</td>\n",
       "      <td>joel35</td>\n",
       "      <td>Summer whole open seat.</td>\n",
       "      <td>anything main news least leader</td>\n",
       "      <td>431</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>nathangalloway</td>\n",
       "      <td>Rock Mr finish sit data in guy up apply recogn...</td>\n",
       "      <td>woman</td>\n",
       "      <td>118</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>butlerandrew</td>\n",
       "      <td>Wind well begin authority school follow.</td>\n",
       "      <td>institution leave hospital writer visit</td>\n",
       "      <td>527</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  post_id            Username  \\\n",
       "0        617      617          angeladiaz   \n",
       "1        835      835        wallacediane   \n",
       "2        554      554  johnsonchristopher   \n",
       "3        569      569            coxbryan   \n",
       "4        802      802              msmith   \n",
       "..       ...      ...                 ...   \n",
       "995      352      352           daviseric   \n",
       "996      953      953      shannonserrano   \n",
       "997      485      485              joel35   \n",
       "998       28       28      nathangalloway   \n",
       "999      193      193        butlerandrew   \n",
       "\n",
       "                                               Caption  \\\n",
       "0    Fall this water will subject stage issue usual...   \n",
       "1               Language family control teach exactly.   \n",
       "2                               Become here recognize.   \n",
       "3    We find according month green able program by ...   \n",
       "4            Car issue current movie during authority.   \n",
       "..                                                 ...   \n",
       "995  Dinner whether girl boy course recognize billi...   \n",
       "996  Son computer myself protect attention city cup...   \n",
       "997                            Summer whole open seat.   \n",
       "998  Rock Mr finish sit data in guy up apply recogn...   \n",
       "999           Wind well begin authority school follow.   \n",
       "\n",
       "                                    Hashtags  Likes  Comments  \n",
       "0                 record his need good whole    517       144  \n",
       "1                    stage animal too of lot    441       182  \n",
       "2                                      story     41       197  \n",
       "3               might decide pattern provide     41       190  \n",
       "4                                     impact    367       164  \n",
       "..                                       ...    ...       ...  \n",
       "995             production main lead example    355       182  \n",
       "996                              east reduce    611        90  \n",
       "997          anything main news least leader    431        27  \n",
       "998                                    woman    118       162  \n",
       "999  institution leave hospital writer visit    527         4  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sentiments.csv')\n",
    "df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis complete! New file saved as 'instagram_sentiments.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Step 1: Load the CSV file\n",
    "df = pd.read_csv('sentiments.csv')\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "# Keep only relevant columns (Caption, Hashtags)\n",
    "df = df[['Caption', 'Hashtags']]\n",
    "\n",
    "# Remove any missing or NaN values\n",
    "df.dropna(subset=['Caption', 'Hashtags'], inplace=True)\n",
    "\n",
    "# Step 3: Sentiment Analysis Function\n",
    "\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity  # Returns a score between -1 and 1\n",
    "\n",
    "\n",
    "# Step 4: Calculate Sentiment Scores for Caption and Hashtags\n",
    "df['caption_score'] = df['Caption'].apply(analyze_sentiment)\n",
    "df['hashtag_score'] = df['Hashtags'].apply(analyze_sentiment)\n",
    "\n",
    "# Step 5: Combine Scores to Determine Overall Sentiment\n",
    "df['overall_score'] = (df['caption_score'] + df['hashtag_score']) / 2\n",
    "\n",
    "# Step 6: Categorize Sentiment\n",
    "\n",
    "\n",
    "def categorize_sentiment(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score > 0:\n",
    "        return 'Positive'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "\n",
    "df['sentiment'] = df['overall_score'].apply(categorize_sentiment)\n",
    "\n",
    "# Step 7: Create a new DataFrame with required columns\n",
    "final_df = df[['Caption', 'Hashtags', 'sentiment', 'overall_score']]\n",
    "\n",
    "# Step 8: Save to a new CSV file\n",
    "final_df.to_csv('instagram_reach_with_sentiments.csv', index=False)\n",
    "\n",
    "print(\"Sentiment analysis complete! New file saved as 'instagram_sentiments.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Caption          1000\n",
       "Hashtags          982\n",
       "sentiment           3\n",
       "overall_score     332\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('instagram_reach_with_sentiments.csv')\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "Positive    520\n",
       "Neutral     248\n",
       "Negative    232\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
