{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data import info_url,posts_url,reels_url,hashtags_url\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collected data from different API calls\n",
    "info_data = requests.get(info, headers=headers, params=querystring).json()\n",
    "posts_data = requests.get(post, headers=headers, params=querystring).json()\n",
    "reels_data = requests.get(reels, headers=headers, params=querystring).json()\n",
    "hashtags_data = requests.get(\n",
    "    hastag, headers=headers, params=querystring).json()\n",
    "\n",
    "# Converting the collected data into DataFrames\n",
    "info_df = pd.DataFrame(info_data['data'])\n",
    "posts_df = pd.DataFrame(posts_data['data'])\n",
    "reels_df = pd.DataFrame(reels_data['data'])\n",
    "hashtags_df = pd.DataFrame(hashtags_data['data'])\n",
    "\n",
    "# Save the datasets if needed\n",
    "info_df.to_csv(\"info_data.csv\", index=False)\n",
    "posts_df.to_csv(\"posts_data.csv\", index=False)\n",
    "reels_df.to_csv(\"reels_data.csv\", index=False)\n",
    "hashtags_df.to_csv(\"hashtags_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "posts_df.drop_duplicates(inplace=True)\n",
    "reels_df.drop_duplicates(inplace=True)\n",
    "hashtags_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values or remove rows with missing data\n",
    "posts_df.fillna('', inplace=True)\n",
    "reels_df.fillna('', inplace=True)\n",
    "hashtags_df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Removing URLs and special characters\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "posts_df['caption'] = posts_df['caption'].apply(clean_text)\n",
    "reels_df['caption'] = reels_df['caption'].apply(clean_text)\n",
    "hashtags_df['hashtags'] = hashtags_df['hashtags'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all datasets into a single DataFrame\n",
    "final_data = pd.merge(posts_df[['caption', 'url']], reels_df[[\n",
    "                      'caption', 'url']], on='url', how='outer')\n",
    "final_data = pd.merge(\n",
    "    final_data, hashtags_df[['hashtags', 'url']], on='url', how='outer')\n",
    "\n",
    "# Combine captions and hashtags for a complete content-based recommendation\n",
    "final_data['content'] = final_data['caption'] + \" \" + final_data['hashtags']\n",
    "\n",
    "# Drop any remaining duplicates or nulls\n",
    "final_data.drop_duplicates(subset='url', inplace=True)\n",
    "final_data.dropna(subset=['content'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional because hamne pahle hi nikala hai\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "def get_sentiment(text):\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    return score['compound']\n",
    "\n",
    "\n",
    "final_data['sentiment'] = final_data['content'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorizing the text content\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(final_data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to recommend based on content\n",
    "def get_recommendations(index, cosine_sim=cosine_sim):\n",
    "    # Get the pairwise similarity scores of all content\n",
    "    sim_scores = list(enumerate(cosine_sim[index]))\n",
    "\n",
    "    # Sort content based on similarity score\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the indices of the top 10 most similar content\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Return the URLs of the most similar content\n",
    "    content_indices = [i[0] for i in sim_scores]\n",
    "    return final_data['url'].iloc[content_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the recommendation system for the first post\n",
    "print(get_recommendations(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "# Save the cosine similarity matrix\n",
    "with open('cosine_similarity.pkl', 'wb') as f:\n",
    "    pickle.dump(cosine_sim, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserID       Username                                            Caption  \\\n",
      "0     463  brandonrogers  Nature reason every phone green approach artic...   \n",
      "1     672       thomas48  Into team prove wide police entire wear everyb...   \n",
      "2     240    longpatrick  Indeed bank cost senior treatment receive bit ...   \n",
      "3     635         clynch         Movement talk need inside opportunity lot.   \n",
      "4     665   michaellopez       Sit over thus ok point family range officer.   \n",
      "\n",
      "                   Hashtags  Likes  Comments  \n",
      "0       marriage bring life    402       196  \n",
      "1                      draw    163       184  \n",
      "2        list school strong    905        40  \n",
      "3  strong money down though    356        82  \n",
      "4                 play news    991        66  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Function to create synthetic dataset\n",
    "\n",
    "\n",
    "def create_synthetic_data(num_samples=1000):\n",
    "    data = {\n",
    "        'UserID': [],\n",
    "        'Username': [],\n",
    "        'Caption': [],\n",
    "        'Hashtags': [],\n",
    "        'Likes': [],\n",
    "        'Comments': []\n",
    "    }\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # Generate user data\n",
    "        user_id = random.randint(0, 1000)\n",
    "        username = fake.user_name()\n",
    "        caption = fake.sentence(nb_words=random.randint(5, 15))\n",
    "        hashtags = ' '.join([fake.word() for _ in range(random.randint(1, 5))])\n",
    "        likes = random.randint(0, 1000)\n",
    "        comments = random.randint(0, 200)\n",
    "\n",
    "        # Append data\n",
    "        data['UserID'].append(user_id)\n",
    "        data['Username'].append(username)\n",
    "        data['Caption'].append(caption)\n",
    "        data['Hashtags'].append(hashtags)\n",
    "        data['Likes'].append(likes)\n",
    "        data['Comments'].append(comments)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Create synthetic dataset\n",
    "synthetic_data = create_synthetic_data(num_samples=1000)\n",
    "\n",
    "# Save to CSV\n",
    "synthetic_data.to_csv('synthetic_instagram_data.csv', index=False)\n",
    "print(synthetic_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Username</th>\n",
       "      <th>Caption</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>463</td>\n",
       "      <td>brandonrogers</td>\n",
       "      <td>Nature reason every phone green approach artic...</td>\n",
       "      <td>marriage bring life</td>\n",
       "      <td>402</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>672</td>\n",
       "      <td>thomas48</td>\n",
       "      <td>Into team prove wide police entire wear everyb...</td>\n",
       "      <td>draw</td>\n",
       "      <td>163</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>240</td>\n",
       "      <td>longpatrick</td>\n",
       "      <td>Indeed bank cost senior treatment receive bit ...</td>\n",
       "      <td>list school strong</td>\n",
       "      <td>905</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>635</td>\n",
       "      <td>clynch</td>\n",
       "      <td>Movement talk need inside opportunity lot.</td>\n",
       "      <td>strong money down though</td>\n",
       "      <td>356</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>665</td>\n",
       "      <td>michaellopez</td>\n",
       "      <td>Sit over thus ok point family range officer.</td>\n",
       "      <td>play news</td>\n",
       "      <td>991</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>72</td>\n",
       "      <td>sbrooks</td>\n",
       "      <td>Who fish same break throughout door reveal tro...</td>\n",
       "      <td>population class us animal</td>\n",
       "      <td>976</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>941</td>\n",
       "      <td>alibrittany</td>\n",
       "      <td>Democrat much student three hear job professor...</td>\n",
       "      <td>hour he western these remain</td>\n",
       "      <td>624</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>837</td>\n",
       "      <td>catherinecunningham</td>\n",
       "      <td>To consider sign garden resource natural start...</td>\n",
       "      <td>stay</td>\n",
       "      <td>244</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>130</td>\n",
       "      <td>amy46</td>\n",
       "      <td>She offer south week evidence along yet that r...</td>\n",
       "      <td>certain what career</td>\n",
       "      <td>785</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>797</td>\n",
       "      <td>tpeterson</td>\n",
       "      <td>Step something or many majority yes.</td>\n",
       "      <td>senior</td>\n",
       "      <td>334</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     UserID             Username  \\\n",
       "0       463        brandonrogers   \n",
       "1       672             thomas48   \n",
       "2       240          longpatrick   \n",
       "3       635               clynch   \n",
       "4       665         michaellopez   \n",
       "..      ...                  ...   \n",
       "995      72              sbrooks   \n",
       "996     941          alibrittany   \n",
       "997     837  catherinecunningham   \n",
       "998     130                amy46   \n",
       "999     797            tpeterson   \n",
       "\n",
       "                                               Caption  \\\n",
       "0    Nature reason every phone green approach artic...   \n",
       "1    Into team prove wide police entire wear everyb...   \n",
       "2    Indeed bank cost senior treatment receive bit ...   \n",
       "3           Movement talk need inside opportunity lot.   \n",
       "4         Sit over thus ok point family range officer.   \n",
       "..                                                 ...   \n",
       "995  Who fish same break throughout door reveal tro...   \n",
       "996  Democrat much student three hear job professor...   \n",
       "997  To consider sign garden resource natural start...   \n",
       "998  She offer south week evidence along yet that r...   \n",
       "999               Step something or many majority yes.   \n",
       "\n",
       "                         Hashtags  Likes  Comments  \n",
       "0             marriage bring life    402       196  \n",
       "1                            draw    163       184  \n",
       "2              list school strong    905        40  \n",
       "3        strong money down though    356        82  \n",
       "4                       play news    991        66  \n",
       "..                            ...    ...       ...  \n",
       "995    population class us animal    976       131  \n",
       "996  hour he western these remain    624       181  \n",
       "997                          stay    244         0  \n",
       "998           certain what career    785       138  \n",
       "999                        senior    334        64  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('synthetic_instagram_data.csv')\n",
    "df.head(1000)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
